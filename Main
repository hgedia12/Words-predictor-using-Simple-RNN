
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

INPUT_FILE = "Dataset_Alice_in_the_wonderland"


# Taking input and making it a stream of character
_input = open(INPUT_FILE, 'rb')
lines = []
for line in _input:
    line = line.strip().lower()                        #making every character lower
    line = line.decode("ascii", "ignore")              #converting the unicode into ASCII
    if len(line) == 0:
        continue
    lines.append(line)                                 #we get lines as a list
_input.close()           
text = " ".join(lines)                                 #we join them
#print(text)


from __future__ import print_function
from keras.layers.recurrent import SimpleRNN
from keras.models import Sequential
from keras.layers import Dense, Activation



# We are making a lookup table which will help us refer the characters while computing

chars = set([c for c in text])                                               #set of characters
nb_chars = len(chars)                                                        #total no. of characters
char_to_index = dict((c, i) for i, c in enumerate(chars))
index_to_char = dict((i, c) for i, c in enumerate(chars))
print(char_to_index)

# - In the next step, we create the input and label texts.


# Here, we are taking an input of 10 previous letters and giving output to 1,thus seqlen=10,step=1
SEQLEN = 10
STEP = 1


#here, we are making every possible sequence of 10 letters and their corrosponding answer

input_chars = []
label_chars = []
for i in range(0, len(text) - SEQLEN, STEP):
    input_chars.append(text[i:i + SEQLEN])
    label_chars.append(text[i + SEQLEN])
#print(input_chars)
#print(label_chars)


# Converting inputs and outputs to One Hot Encoding to use them in our Neural Network,(That is why we made the lookup table)
X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)              #Creating a matrix which contains the one hot encoded input
y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)                      #Creating a matrix which contains the one hot encoded output
for i, input_char in enumerate(input_chars):
    #print(input_char,i)
    for j, ch in enumerate(input_char):
        X[i, j, char_to_index[ch]] = 1                                            #Making the respective index of each letter one(That's how OHE works)
        #print(ch,j)
    y[i, char_to_index[label_chars[i]]] = 1

# Build the model.
# Here we classify all the parameters beforehand, so that changing them is easy

HIDDEN_SIZE = 128
BATCH_SIZE = 128
NUM_ITERATIONS = 25
NUM_EPOCHS_PER_ITERATION = 1
NUM_PREDS_PER_EPOCH = 100

model = Sequential()
model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,
                    input_shape=(SEQLEN, nb_chars),
                    unroll=True))
model.add(Dense(nb_chars))
model.add(Dense(nb_chars))
model.add(Activation("softmax"))

model.compile(loss="categorical_crossentropy", optimizer="rmsprop")


#Training model
for iter in range(NUM_ITERATIONS):
    print("=" * 50)
    print("Iteration #: %d" % (iter))
    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)
    
    # To test our model, we randomly choose a row from input_chars, then use it to generate text from model for next 100 chars
    test_idx = np.random.randint(len(input_chars))
    test_chars = input_chars[test_idx]
    print("Generating from seed: %s" % (test_chars))
    print(test_chars, end="")
    for i in range(NUM_PREDS_PER_EPOCH):
        Xtest = np.zeros((1, SEQLEN, nb_chars))
        for i, ch in enumerate(test_chars):
            Xtest[0, i, char_to_index[ch]] = 1
        pred = model.predict(Xtest, verbose=0)[0]
        ypred = index_to_char[np.argmax(pred)]
        print(ypred, end="")
        # move forward with test_chars + ypred
        test_chars = test_chars[1:] + ypred
    print()
